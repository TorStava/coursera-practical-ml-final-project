---
title: "Practical machine learning - course project"
author: "Tor Olav Stava"
date: "24 mars 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive summary

In this report we will predict the quality of performance of weight lifting exercises based on the weight lifting dataset found here: (http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). We create five prediction models using different methods, and compare the resulting accuracy against each other. The results show that the random forest and boosted trees methods have the best accuracy, and random forest taking a marginal lead. We therefore use the random forest model for the final validation, and the results from this prediction is submitted for grading. The final prediction on the validation set had a 100% accuracy.

## Load libraries and datafiles

```{r message = FALSE}
# Load libraries
library(caret)
library(e1071)

# Set working directory
setwd("I:/Dropbox/Coursera/practical-machine-learning")

# Load datafiles, assume they exist in the working directory
pml_training <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
pml_testing <- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
```

## Data Cleaning

Before we can start training our models we need to get rid of data columns containing 'NA' values. We also remove the first three columns, since they are not relevant for the accelerometer data.

```{r}
# Create index of columns containing 'NA' values
naIndex <- sapply(pml_training, function(x) any(is.na(x)))

# Get rid of the columns identified above in both the training and the testing sets
pml_training <- pml_training[, !naIndex]
pml_testing <- pml_testing[, !naIndex]

# Finally, get rid of the first columns since they could lead to overfitting (names, etc..)
pml_training <- pml_training[, -c(1:6)]
pml_testing <- pml_testing[, -c(1:6)]
```

## Splitting the training data set

Now, we split the training data set into separate training and test sets, then use the supplied test set as the final validation data set.

```{r}
set.seed(1234) # Set the seed for reproducability

trainIndex <- createDataPartition(y = pml_training$classe, p = 0.8, list = FALSE)
Training <- pml_training[trainIndex, ]
Testing <- pml_training[-trainIndex, ]
Validation <- pml_testing
```


## Building prediction models on the training data set

We'll train five prediction models using different methods.

```{r cache = TRUE}
fitDT <- train(classe ~ ., data = Training, method = "rpart") # Decision Tree
fitRF <- train(classe ~ ., data = Training, method = "rf") # Random Forest
fitGBM <- train(classe ~ ., data = Training, method = "gbm", verbose = FALSE) # Boosted Trees
fitLDA <- train(classe ~ ., data = Training, method = "lda") # Linear Discriminant Analysis
fitSVM <- svm(classe ~ ., data = Training) # Support Vector Machines
```

## Predictions

Now, we'll use our testing data set to test our trained models to deterimine the accuracy of the different models.

```{r}
predDT <- predict(fitDT, newdata = Testing)
cfmDT <- confusionMatrix(predDT, Testing$classe)

predRF <- predict(fitRF, newdata = Testing)
cfmRF <- confusionMatrix(predRF, Testing$classe)

predSVM <- predict(fitSVM, newdata = Testing)
cfmSVM <- confusionMatrix(predSVM, Testing$classe)

predGBM <- predict(fitGBM, newdata = Testing)
cfmGBM <- confusionMatrix(predGBM, Testing$classe)

predLDA <- predict(fitLDA, newdata = Testing)
cfmLDA <- confusionMatrix(predSVM, Testing$classe)

t(data.frame(
        Decision.Tree = round(cfmDT$overall['Accuracy'], 4),
        Random.Forest = round(cfmRF$overall['Accuracy'], 4),
        Support.Vector.Machines = round(cfmSVM$overall['Accuracy'], 4),
        Boosted.Trees = round(cfmGBM$overall['Accuracy'], 4),
        Linear.Discriminant.Analysis = round(cfmLDA$overall['Accuracy'], 4)
        )
)
```

From the results we can see that the decision tree method performs rather poorly with only `r round(cfmDT$overall['Accuracy']*100, 1)`% accuracy, while random forest and boosted trees performs exceptionally well on the testing set with `r round(cfmRF$overall['Accuracy']*100, 1)`% and `r round(cfmGBM$overall['Accuracy']*100, 1)`% accuracy, respectively. The best performing model is random forest, so we'll select this for our final model.

## Validating the prediction model

Now, let's validate our model on the validation set. The results from this validation run will be submitted to the Coursera grading quiz.

```{r}
valRF <- predict(fitRF, newdata = Validation)
print(valRF)
```